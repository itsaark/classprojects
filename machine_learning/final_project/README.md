# Text to Image Synthesis

Using NLP feature extraction, and Generative Adversarial Networks to generate images from text descriptions.  

Partial replication of the paper [Generative Adversarial Text to Image Synthesis](https://arxiv.org/pdf/1605.05396.pdf).  
Authors: [Scott Reed](https://github.com/reedscot), Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee  

## Data sets

### General Use  

From Caltech and Oxford websites:
 * [Caltech-UCSD Birds-200-2011](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)   

### Preprocessed w/Text descriptions  

From primary author's [github repo](https://github.com/reedscot/icml2016):
 * [Birds from github.com/reedscot/cvpr2016](https://drive.google.com/file/d/0B0ywwgffWnLLZW9uVHNjb2JmNlE/view)
 * [Flowers from github.com/reedscot/cvpr2016](https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view)

### Our experiments and results
 In order to standardize our experiments we set some hyper parameters constant across all the experiments. We set our learning rate to 0.004, beta value to 0.3 and batch size to 128. Although the paper they ran 1,000 epochs for each experiment, due to computing limits we ran our experiments for 100 epochs.

*  Word2Vec with stop words
 We generated word embeddings for all the words in an input sentence using the Word2Vec algorithm and we then took mean of all the vectors to generate an embedding for the whole sentence. Each of these sentence vectors is of size 1024. The discriminator loss slowly tends to hit zero and gets better at predicting. But the discriminator tends to be be having lots spikes jumping up to as high as  17.

 ![alt text](img\stopwords.png)
 ![alt text](img\stopwords_g.png)


* Word2Vec without stop words
 For this experiment we used the set of stopwords defined in the NLTK library and ignored all of those while computing the mean of the word vectors. We saw pretty good improvements in the images which were generated after the last epoch. Although the generator loss has some spikes, overall it tends to converge.

 ![alt text](\img\without.png)
 ![alt text](\img\without_g.png)

* Char CNN-RNN
 For this experiment we used pre-trained text encodings which were made available by the authors of the paper. They used a character level CNN-RNN to generate the word embeddings. We produced some high quality images using their pre-trained word embeddings, but we did notice some unusual spikes in the loss function plot of generator. We suppose it’s common to have such usual spikes early on in training. Since we couldn’t run our experiments for the full 1,000 epochs (due to time constraint), we weren’t able to generate a plot where both discriminator and generator losses converge.

 ![alt text](\img\charcnn.png)
 ![alt text](\img\charcnn_g.png)

* Presentation Demo
  This was the sample image generated by our model which was part the of class demonstration. The input sentence for this was “this bird is yellow in color, with a short beak and a light brown eye ring.”

  ![alt text](\img\pres.png)
